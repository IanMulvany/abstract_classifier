{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need to try to extract training data. \n",
    "- need to create a holdout to use to test the predictions with, can be a small holdout \n",
    "- there are possibly two different patterns - files from Christine \n",
    "- files that have been added since maybe the 4th of March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from xlrd import open_workbook\n",
    "import os\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_files = glob.glob(\"/Users/ianm/Dropbox/workbench/sage-maker-srm-test/excel-data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(excel_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â get files modified on or after the 4th of March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_date(path_to_file):\n",
    "    \"\"\"\n",
    "    Try to get the date that a file was created, falling back to when it was\n",
    "    last modified if that isn't possible.\n",
    "    See http://stackoverflow.com/a/39501288/1709587 for explanation.\n",
    "    \"\"\"\n",
    "    stat = os.stat(path_to_file)\n",
    "    try:\n",
    "#         return stat.st_birthtime\n",
    "        return stat.st_mtime\n",
    "    except AttributeError:\n",
    "        # We're probably on Linux. No easy way to get creation dates here,\n",
    "        # so we'll settle for when its content was last modified.\n",
    "        return stat.st_mtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_epoch_time = 1551653455 # this is unix time for 3rd of March 2019, 23:00 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_mar_four_files = []\n",
    "for file in excel_files:\n",
    "    mod_time = creation_date(file)\n",
    "    if mod_time > selection_epoch_time:\n",
    "        post_mar_four_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'post_mar_four_files' (list)\n"
     ]
    }
   ],
   "source": [
    "%store post_mar_four_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get files from a team member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_from_team_member(initials, file_list):\n",
    "    team_member_files = []\n",
    "    for file in file_list:\n",
    "        tail_name = file.split(\"/\")[-1]\n",
    "        if tail_name.split()[0] == initials:\n",
    "            team_member_files.append(file)\n",
    "    return team_member_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get files from Christine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "christine_files = files_from_team_member(\"CS\", excel_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic xlrd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_workbook(filename):\n",
    "    wb = open_workbook(filename)\n",
    "    return wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worksheet(filename, index):\n",
    "    wb = open_workbook(filename)\n",
    "    sheet = wb.sheets()[index]\n",
    "    return sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sheet_names(wb):\n",
    "    return wb.sheet_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_names(sheet):\n",
    "    index_row = 0\n",
    "    col_names = row = sheet.row(index_row)\n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get abstract and titles of selected and rejected articles from Christine files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names_with_selections = [\"contenders\", \"to email\", \"names to check for email\", \"double articles- manual email\", \"Contenders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_col_names = [\"AB\", \"Abstract\"]\n",
    "title_col_names = [\"TI\", \"Title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sheet_names_against_validation(sheet_names, validation_list):\n",
    "    for name in sheet_names:\n",
    "        if name in validation_list: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the sheets have to contain both \"original\" and something from a selection list. \n",
    "Then we pull the abstracts from \"original\" and from one of the other columns too. \n",
    "we could just naievely run this twice and have simpler logic, I think that wojld be better\n",
    "so \n",
    "for each file\n",
    "    get the candidate colum\n",
    "    then get the original colum\n",
    "    and if we don't have both skip\n",
    "    else add to the aggregate list! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_values_by_col_name(sheet, col_name_candidates):\n",
    "    \"\"\"\n",
    "    the assumption is that we are looking for a colum from one sheet\n",
    "    but the sheet and colum name might have one of a number of differnet values, but only one of them. \n",
    "    \"\"\"\n",
    "    col_names = get_col_names(sheet)\n",
    "    for index, col in enumerate(col_names):\n",
    "        if col.value in col_name_candidates:\n",
    "            col_values = sheet.col_values(index)\n",
    "            return col_values\n",
    "    return None # if we don't find the column return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_values_by_col_name_sheet_name(workbook, sheet_name_candidates, col_name_candidates):\n",
    "    \"\"\"\n",
    "    the assumption is that we are looking for a colum from one sheet\n",
    "    but the sheet and colum name might have one of a number of differnet values, but only one of them. \n",
    "    \"\"\"\n",
    "    sheet_names = get_sheet_names(wb)\n",
    "    for index, sheet_name in enumerate(sheet_names):\n",
    "        if sheet_name in sheet_name_candidates:\n",
    "            sheet = wb.sheet_by_name(sheet_name)\n",
    "            col_values = get_col_values_by_col_name(sheet, col_name_candidates)\n",
    "            return col_values\n",
    "    return None # if we don't find the sheet, return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "175\n",
      "\n",
      "427\n",
      "369\n",
      "\n",
      "991\n",
      "707\n",
      "\n",
      "1082\n",
      "720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_test_abstracts = []\n",
    "all_selected_abstracts = []\n",
    "for file in christine_files:\n",
    "    wb = get_workbook(file)\n",
    "    original_sheet_candidates = [\"original\"]\n",
    "    original_abstracts = get_col_values_by_col_name_sheet_name(wb, original_sheet_candidates, abstract_col_names)\n",
    "    selected_abstracts = get_col_values_by_col_name_sheet_name(wb, sheet_names_with_selections, abstract_col_names)\n",
    "    if original_abstracts and selected_abstracts:\n",
    "        all_test_abstracts.extend(original_abstracts)\n",
    "        all_selected_abstracts.extend(selected_abstracts)\n",
    "        print(len(all_test_abstracts))\n",
    "        print(len(all_selected_abstracts))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715\n"
     ]
    }
   ],
   "source": [
    "print(len(list(set(all_selected_abstracts))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_abstracts= list(set(all_test_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_selected_abstracts = list(set(all_selected_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_abstracts = []\n",
    "for abstract in unique_abstracts:\n",
    "    if abstract not in unique_selected_abstracts:\n",
    "        rejected_abstracts.append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071\n",
      "715\n",
      "356\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_abstracts))\n",
    "print(len(unique_selected_abstracts))\n",
    "print(len(rejected_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'unique_selected_abstracts' (list)\n",
      "Stored 'rejected_abstracts' (list)\n"
     ]
    }
   ],
   "source": [
    "%store unique_selected_abstracts\n",
    "%store rejected_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scramble our lists and get our holdouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(unique_selected_abstracts)\n",
    "random.shuffle(rejected_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLDOUT_NUMBER = 100\n",
    "holdout_selected_abstracts = unique_selected_abstracts[0:HOLDOUT_NUMBER]\n",
    "training_selected_abstracts = unique_selected_abstracts[HOLDOUT_NUMBER:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715\n",
      "100\n",
      "615\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_selected_abstracts))\n",
    "print(len(holdout_selected_abstracts))\n",
    "print(len(training_selected_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Unknown variable 'unique_selected_abstracts'\n"
     ]
    }
   ],
   "source": [
    "%store unique_selected_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Background: Rheumatic heart disease (RHD) among Ethiopian school children was recently found to be 1.4%. Immigration of the Jewish population from the Gondar region to Israel created an opportunity for further enquiry. Methods: A cross-sectional study of the cardiac status of 113,671 adolescent recruits aged 16-19 years from the northern district of Israel who completed the medical profiling process over a 22-year period. Results: 140 recruits had a history of rheumatic fever (0.12%), although none from an Ethiopian origin (n = 1,719). The prevalence of valvular heart disease clinically and confirmed echocardiographically in Ethiopian recruits was not different from the total population (0.81 and 0.93%, respectively). However, the prevalence was higher in those migrating to Israel in their 13th year or older (2.09%), compared to those migrating at a younger age or born in Israel (0.49%). Conclusion: The Ethiopian teenage Israeli population from Gondar had a high rate of auscultation positive and echocardiographically confirmed valvular disease that suggested a high rate of RHD (similar to 1.6%), despite no relevant past history. Our findings also suggested that for the younger Ethiopian immigrants or Israeli born subjects of Ethiopian origin, the improved medical care may well reduce the prevalence of valvular heart disease to that of the rest of the local population.',\n",
       " \"Falls among older adults result in substantial morbidity and mortality. Community-based programs have been shown to decrease the rate of falls. In 2007, the Centers for Disease Control and Prevention funded a research study to determine how to successfully disseminate the evidence-based fall prevention program (Stepping On) in the community setting. As the first step for this study, a panel of subject matter experts was convened to suggest which parts of the Stepping On fall prevention program were considered key elements, which could not be modified by implementers. Methods: Older adult fall prevention experts from the US, Canada, and Australia participated in a modified Delphi technique process to suggest key program elements of Stepping On. Forty-four experts were invited to ensure that the panel of experts would consist of equal numbers of physical therapists, occupational therapists, geriatricians, exercise scientists, and public health researchers. Consensus was determined by percent of agreement among panelists. A Rasch analysis of item fit was conducted to explore the degree of diversity and/or homogeneity of responses across our panelists. Results: The Rasch analysis of the 19 panelists using fit statistics shows there was a reasonable and sufficient range of diverse perspectives (Infit MnSQ 1.01, Z score -0.1, Outfit MnSQ 0.96, Z score -0.2 with a separation of 4.89). Consensus was achieved that these elements were key: 17 of 18 adult learning elements, 11 of 22 programming, 12 of 15 exercise, 7 of 8 upgrading exercises, 2 of 4 peer co-leader's role, and all of the home visits, booster sessions, group leader's role, and background and training of group leader elements. The top five key elements were: (1) use plain language, (2) develop trust, (3) engage people in what is meaningful and contextual for them, (4) train participants for cues in self-monitoring quality of exercises, and (5) group leader learns about exercises and understands how to progress them. Discussion: The Delphi consensus process suggested key elements related to Stepping On program delivery. These elements were considered essential to program effectiveness. Findings from this study laid the foundation for translation of Stepping On for broad US dissemination.\",\n",
       " \"Gun violence is related to substantial morbidity and mortality with surrounding discussions framed and shaped by the media. This study's objective was to explore national news media's reporting of gun violence around a mass shooting. National news pieces were coded according to categories of gun violence, media frames, entities held responsible, responses, and reporting of the public heath approach. Individuals were held responsible for gun violence in 63% of pieces before and 32% after the shooting. Lawmakers were held responsible in 30% of pieces before and 66% after. Background checks were a proposed gun violence prevention method in 18% of pieces before and 55% after Sandy Hook, and lethality reduction of firearms was in 9% before and 57% after. Following a mass shooting, the media tended to hold government, not individuals, primarily responsible. The media often misrepresented the real picture of gun violence and key public health roles.\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_selected_abstracts[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have a couple of duplicate abstracts in our data, boo\n",
    "\n",
    "we have about 1015 uniqe abstracts, of which 715 have been selected.\n",
    "\n",
    "need to pick out a set as a holdout to test the trainig algorithim with \n",
    "\n",
    "I'm going to pick 100 selected, and 100 non selected abstracts as my holdout, I'm also going to randomize the selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Next steps:  \n",
    "\n",
    "- learn about what format SAGE maker needs for training \n",
    "- work though the two lists creating a tagged set of the format that SAGE maker needs  \n",
    "- take out a 100 item set for both the selected and unselcted ones. \n",
    "- shuffle the training set  \n",
    "- send the training set to SAGE maker   \n",
    "- send the holdouts to SAGE maker to see how the model preforms  \n",
    "- repeat the above for titles "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
